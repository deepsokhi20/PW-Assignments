{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e45bc4a6-b89a-4257-ac76-f058ac1b445e",
   "metadata": {},
   "source": [
    "#Q1\n",
    "\n",
    "Polynomial functions and kernel functions are both mathematical tools used in machine learning algorithms, particularly in the context of non-linear transformations of input data. There is a close relationship between polynomial functions and kernel functions, especially in the context of kernel methods such as Support Vector Machines (SVMs).\n",
    "\n",
    "1. **Polynomial Functions**:\n",
    "   - Polynomial functions are mathematical functions of the form \\( f(x) = a_n x^n + a_{n-1} x^{n-1} + \\ldots + a_1 x + a_0 \\), where \\( x \\) is the variable, \\( n \\) is the degree of the polynomial, and \\( a_0, a_1, \\ldots, a_n \\) are the coefficients. \n",
    "   - Polynomial functions can be used to transform input data into a higher-dimensional space. For example, a polynomial kernel in SVM applies polynomial functions to the input features, transforming them into a higher-dimensional space where the data might be more separable.\n",
    "\n",
    "2. **Kernel Functions**:\n",
    "   - Kernel functions, in the context of kernel methods like SVMs, are similarity functions that measure the similarity or distance between pairs of data points in the original feature space. Common kernel functions include linear, polynomial, radial basis function (RBF), and sigmoid kernels.\n",
    "   - Polynomial kernel functions are a specific type of kernel function that computes the dot product of two feature vectors raised to a certain power, which effectively captures the similarity between data points in a higher-dimensional space without explicitly computing the transformation.\n",
    "\n",
    "3. **Relationship**:\n",
    "   - Polynomial functions and polynomial kernel functions are closely related. Polynomial kernel functions effectively compute the dot product of transformed feature vectors in a higher-dimensional space, where the transformation is achieved using polynomial functions.\n",
    "   - By using polynomial kernel functions, it is unnecessary to explicitly compute the transformation of input data into a higher-dimensional space using polynomial functions. Instead, the kernel function implicitly represents the similarity between data points in that space.\n",
    "   - This relationship allows kernel methods like SVMs to handle non-linearly separable data by effectively operating in a higher-dimensional space defined by polynomial transformations without the need for explicit feature mapping.\n",
    "\n",
    "In summary, while polynomial functions and polynomial kernel functions serve similar purposes in terms of transforming data into higher-dimensional spaces, polynomial kernel functions offer a more efficient and computationally feasible approach by implicitly capturing the similarity between data points in the transformed space without explicitly computing the transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a98ec6-6aad-4044-86c3-3299923dc2a3",
   "metadata": {},
   "source": [
    "#Q2\n",
    "\n",
    "To implement a Support Vector Machine (SVM) with a polynomial kernel in Python using Scikit-learn, you can use the `SVC` (Support Vector Classifier) class and specify the kernel parameter as `'poly'`. Additionally, you can tune other hyperparameters such as the degree of the polynomial kernel, regularization parameter \\( C \\), and the coefficient \\( \\gamma \\) (if applicable). Here's an example:\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM classifier with a polynomial kernel\n",
    "degree = 3  # Degree of the polynomial kernel\n",
    "C = 1.0  # Regularization parameter\n",
    "svm_classifier = SVC(kernel='poly', degree=degree, C=C)\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model on the testing set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "```\n",
    "\n",
    "In this example:\n",
    "- We first load the Iris dataset and split it into training and testing sets.\n",
    "- Then, we create an SVM classifier using the `SVC` class and specify the kernel parameter as `'poly'` to indicate a polynomial kernel.\n",
    "- We can optionally specify the degree of the polynomial kernel using the `degree` parameter (default is 3).\n",
    "- Other hyperparameters like the regularization parameter \\( C \\) can be tuned using the `C` parameter.\n",
    "- Next, we train the SVM classifier on the training set using the `fit` method.\n",
    "- After training, we use the trained classifier to predict the labels for the testing set using the `predict` method.\n",
    "- Finally, we compute the accuracy of the model on the testing set using the `accuracy_score` function from Scikit-learn.\n",
    "\n",
    "This code demonstrates how to implement an SVM with a polynomial kernel in Python using Scikit-learn and apply it to the Iris dataset for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3605bd2f-8eb8-494f-bc4e-a9826dfa9a46",
   "metadata": {},
   "source": [
    "#Q3\n",
    "\n",
    "In Support Vector Regression (SVR), the parameter \\( \\epsilon \\) controls the width of the margin within which no penalty is associated with errors. Increasing the value of \\( \\epsilon \\) allows for a wider margin, meaning that data points can be further from the predicted function while still being considered within the margin of tolerance.\n",
    "\n",
    "The relationship between the value of \\( \\epsilon \\) and the number of support vectors in SVR is as follows:\n",
    "\n",
    "1. **Increasing \\( \\epsilon \\) may decrease the number of support vectors**:\n",
    "   - When \\( \\epsilon \\) is increased, the margin becomes wider, allowing more data points to fall within the margin of tolerance without incurring a penalty.\n",
    "   - As a result, the SVR model may require fewer support vectors to define the margin and achieve the desired level of accuracy.\n",
    "   - Data points that were previously close to the margin may now fall comfortably within the wider margin and no longer need to be considered as support vectors.\n",
    "\n",
    "2. **Decreasing \\( \\epsilon \\) may increase the number of support vectors**:\n",
    "   - Conversely, when \\( \\epsilon \\) is decreased, the margin becomes narrower, requiring data points to be closer to the predicted function to avoid incurring a penalty.\n",
    "   - In this case, more data points may be required as support vectors to define the narrower margin and maintain the required level of accuracy.\n",
    "   - Data points that were previously within the wider margin may now fall outside the narrower margin and need to be considered as support vectors to ensure that the model captures their influence on the predicted function accurately.\n",
    "\n",
    "In summary, increasing the value of \\( \\epsilon \\) in SVR typically leads to a wider margin and may decrease the number of support vectors, while decreasing \\( \\epsilon \\) leads to a narrower margin and may increase the number of support vectors. The choice of \\( \\epsilon \\) should be made based on the desired trade-off between model complexity and accuracy, with consideration of factors such as the dataset size, noise level, and the degree of tolerance for errors in the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c59e6f-138a-49bf-a35c-f740d097a129",
   "metadata": {},
   "source": [
    "#Q4\n",
    "\n",
    "Support Vector Regression (SVR) is a powerful regression technique that relies on several key parameters to achieve optimal performance. Let's discuss how each parameter affects SVR's performance and provide examples of when you might want to increase or decrease its value:\n",
    "\n",
    "1. **Kernel Function**:\n",
    "   - The kernel function determines the type of mapping applied to the input features to transform them into a higher-dimensional space. Common kernel functions include linear, polynomial, radial basis function (RBF), and sigmoid.\n",
    "   - The choice of kernel function affects the flexibility and non-linearity of the SVR model.\n",
    "   - Example: If the relationship between input features and the target variable is highly non-linear, using a non-linear kernel function such as RBF or polynomial may improve the model's performance.\n",
    "\n",
    "2. **C Parameter**:\n",
    "   - The C parameter controls the trade-off between maximizing the margin and minimizing the training error. A smaller value of C encourages a wider margin at the expense of allowing more training errors, while a larger value of C imposes a stricter penalty for errors, potentially leading to a narrower margin.\n",
    "   - Increasing C may lead to a more complex model that fits the training data more closely, but it may also increase the risk of overfitting.\n",
    "   - Example: If the training data contains noise or outliers, using a smaller value of C may help the model generalize better by allowing for a wider margin and reducing the influence of individual data points.\n",
    "\n",
    "3. **Epsilon Parameter**:\n",
    "   - The epsilon parameter (\\( \\epsilon \\)) defines the margin of tolerance within which no penalty is associated with errors. It controls the width of the tube around the regression line within which data points are considered to be accurately predicted.\n",
    "   - Increasing \\( \\epsilon \\) allows for a wider tube, meaning that data points can be further from the regression line while still being considered accurately predicted.\n",
    "   - Example: In scenarios where there is a higher degree of noise in the target variable or where a certain level of tolerance for errors is acceptable, increasing \\( \\epsilon \\) can help improve the model's robustness to noise and outliers.\n",
    "\n",
    "4. **Gamma Parameter**:\n",
    "   - The gamma parameter (\\( \\gamma \\)) is specific to certain kernel functions like RBF and defines the kernel coefficient. It determines the influence of a single training example, with low values indicating a wider influence and high values indicating a narrower influence.\n",
    "   - A smaller value of \\( \\gamma \\) leads to a smoother decision boundary, while a larger value of \\( \\gamma \\) results in a more complex and potentially overfitting decision boundary.\n",
    "   - Example: When dealing with a large dataset or when the relationship between input features and the target variable is relatively simple, using a smaller value of \\( \\gamma \\) may help prevent overfitting and improve generalization.\n",
    "\n",
    "In summary, the choice of kernel function, C parameter, epsilon parameter, and gamma parameter significantly affects the performance of SVR. Understanding how each parameter works and when to increase or decrease its value is crucial for fine-tuning the SVR model and achieving optimal regression results, depending on the characteristics of the dataset and the problem at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82d9bec7-f657-420b-8520-c9127a0eae6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Best parameters: {'C': 10, 'gamma': 'scale', 'kernel': 'linear'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['svm_classifier.pkl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Assignment\n",
    "# Importing necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data (scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "svc_classifier = SVC()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svc_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = svc_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance of the classifier (accuracy)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Tune the hyperparameters using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf', 'poly'], 'gamma': ['scale', 'auto']}\n",
    "grid_search = GridSearchCV(svc_classifier, param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "tuned_classifier = grid_search.best_estimator_\n",
    "tuned_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Save the trained classifier to a file\n",
    "joblib.dump(tuned_classifier, 'svm_classifier.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1d5974-e866-401e-aa80-c393784b259a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
