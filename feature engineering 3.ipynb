{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1ccd30c-892b-4d24-a75b-0a633c401b8c",
   "metadata": {},
   "source": [
    "#Q1\n",
    "\n",
    "There is another way of data scaling, where the minimum of feature is made equal to zero and the maximum of feature equal to one. MinMax Scaler shrinks the data within the given range, usually of 0 to 1. It transforms data by scaling features to a given range. It scales the values to a specific value range without changing the shape of the original distribution.\n",
    "\n",
    "The MinMax scaling is done using:\n",
    "\n",
    "x_std = (x – x.min(axis=0)) / (x.max(axis=0) – x.min(axis=0))\n",
    "\n",
    "x_scaled = x_std * (max – min) + min\n",
    "\n",
    "Where,\n",
    "\n",
    "min, max = feature_range\n",
    "x.min(axis=0) : Minimum feature value\n",
    "x.max(axis=0):Maximum feature value\n",
    "Sklearn preprocessing defines MinMaxScaler() method to achieve this.\n",
    "\n",
    "Syntax: class sklearn.preprocessing.MinMaxScaler(feature_range=0, 1, *, copy=True, clip=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7970bd9d-d7f1-447f-a36c-48e6a1feab32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.        ]\n",
      " [0.27272727 0.625     ]\n",
      " [0.         1.        ]\n",
      " [1.         0.75      ]]\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    " \n",
    "# create data\n",
    "data = [[11, 2], [3, 7], [0, 10], [11, 8]]\n",
    " \n",
    "# scale features\n",
    "scaler = MinMaxScaler()\n",
    "model=scaler.fit(data)\n",
    "scaled_data=model.transform(data)\n",
    " \n",
    "# print scaled features\n",
    "print(scaled_data)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf7c121-b0a5-4446-929a-470697cf2b0f",
   "metadata": {},
   "source": [
    "#Q2\n",
    "\n",
    "The Unit Vector technique in feature scaling, also known as vector normalization or unit normalization, is a method used to scale the features of a dataset in such a way that they have a magnitude (length) of 1. This means that each feature is transformed to have a Euclidean norm (L2 norm) of 1. The purpose of this technique is to ensure that all features have equal importance or influence on the machine learning model, and it is particularly useful when you have features with different scales and you want to remove the bias introduced by the scale.\n",
    "\n",
    "The formula for calculating the unit vector for a feature is:\n",
    "\n",
    "\\[X_{\\text{unit}} = \\frac{X - \\mu}{\\|X - \\mu\\|}\\]\n",
    "\n",
    "Where:\n",
    "- \\(X_{\\text{unit}}\\) is the unit-scaled feature.\n",
    "- \\(X\\) is the original feature.\n",
    "- \\(\\mu\\) is the mean of the feature.\n",
    "- \\(\\|X - \\mu\\|\\) is the Euclidean norm (L2 norm) of the feature.\n",
    "\n",
    "The Unit Vector technique ensures that all features fall on the unit circle in a multi-dimensional space.\n",
    "\n",
    "In contrast, Min-Max scaling (also known as normalization) scales features to a specific range, typically between 0 and 1. It uses the following formula:\n",
    "\n",
    "\\[X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\\]\n",
    "\n",
    "Where:\n",
    "- \\(X_{\\text{scaled}}\\) is the scaled feature.\n",
    "- \\(X\\) is the original feature.\n",
    "- \\(X_{\\text{min}}\\) is the minimum value of the feature.\n",
    "- \\(X_{\\text{max}}\\) is the maximum value of the feature.\n",
    "\n",
    "The main difference between Unit Vector scaling and Min-Max scaling is in how the scaling is performed. Unit Vector scaling normalizes features so that they have a magnitude of 1, while Min-Max scaling maps features to a specific range (0 to 1 by default). Unit Vector scaling is especially useful when the direction of the data points in multi-dimensional space is more important than their absolute values.\n",
    "\n",
    "Here's an example to illustrate the application of Unit Vector scaling:\n",
    "\n",
    "Suppose you have a dataset with two features, \"Income\" (measured in thousands of dollars) and \"Age\" (measured in years), and you want to scale them using Unit Vector scaling:\n",
    "\n",
    "1. Calculate the mean of each feature (Income and Age).\n",
    "2. Calculate the Euclidean norm (L2 norm) of each feature using the mean.\n",
    "\n",
    "For \"Income\":\n",
    "- Mean of Income (μ_income) = $50,000\n",
    "- Euclidean norm of Income (|Income - μ_income|) = $30,000 (for example)\n",
    "\n",
    "For \"Age\":\n",
    "- Mean of Age (μ_age) = 35 years\n",
    "- Euclidean norm of Age (|Age - μ_age|) = 10 years (for example)\n",
    "\n",
    "3. Apply the Unit Vector scaling formula to each feature:\n",
    "\n",
    "For \"Income\":\n",
    "\\[Income_{\\text{unit}} = \\frac{Income - μ_income}{|Income - μ_income|} = \\frac{Income - 50,000}{30,000}\\]\n",
    "\n",
    "For \"Age\":\n",
    "\\[Age_{\\text{unit}} = \\frac{Age - μ_age}{|Age - μ_age|} = \\frac{Age - 35}{10}\\]\n",
    "\n",
    "Now, both \"Income\" and \"Age\" have been scaled to have a magnitude of 1, making them equally important in a machine learning model regardless of their original scales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2c9c40-4a30-48ff-a3c4-17f0c9e3303e",
   "metadata": {},
   "source": [
    "#Q3\n",
    "\n",
    "PCA, which stands for Principal Component Analysis, is a dimensionality reduction technique commonly used in data analysis and machine learning. Its primary purpose is to transform a dataset with potentially many correlated features (variables) into a new dataset with fewer, uncorrelated features while preserving as much of the original variance as possible. This reduction in dimensionality is particularly useful for simplifying complex datasets, reducing computational costs, and mitigating the curse of dimensionality.\n",
    "\n",
    "Here's a step-by-step explanation of how PCA works and its application:\n",
    "\n",
    "1. **Center the Data**: Before applying PCA, it's essential to center the data by subtracting the mean from each feature. This step ensures that the first principal component represents the direction of maximum variance.\n",
    "\n",
    "2. **Compute the Covariance Matrix**: PCA involves linear transformations, and the covariance matrix is used to understand how features are related to each other. The covariance between two features gives insights into their relationship. The covariance matrix summarizes these relationships.\n",
    "\n",
    "3. **Calculate the Eigenvectors and Eigenvalues**: The eigenvectors of the covariance matrix represent the directions of maximum variance in the data, and the eigenvalues indicate the amount of variance explained by each eigenvector. Typically, you calculate the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "4. **Select Principal Components**: Sort the eigenvalues in descending order. The eigenvector corresponding to the highest eigenvalue is the first principal component, the one with the second-highest eigenvalue is the second principal component, and so on. You can choose how many principal components to keep based on how much variance you want to retain. A common approach is to keep enough components to explain a certain percentage of the total variance (e.g., 95% or 99%).\n",
    "\n",
    "5. **Project Data onto Principal Components**: Transform the original data by projecting it onto the selected principal components. This transformation reduces the dimensionality of the data while preserving the most important information.\n",
    "\n",
    "Here's an example to illustrate PCA's application:\n",
    "\n",
    "Suppose you have a dataset of customer purchase behavior in a retail store with various features like \"Total Spending,\" \"Number of Items Purchased,\" \"Average Purchase Value,\" and \"Frequency of Visits.\" You want to perform dimensionality reduction with PCA.\n",
    "\n",
    "1. **Center the Data**: Subtract the mean of each feature from the data.\n",
    "\n",
    "2. **Compute the Covariance Matrix**: Calculate the covariance matrix to understand how these features are related.\n",
    "\n",
    "3. **Calculate the Eigenvectors and Eigenvalues**: Find the eigenvectors and eigenvalues of the covariance matrix. The eigenvalues represent the variance explained by each principal component, and the eigenvectors represent the directions in which the data varies the most.\n",
    "\n",
    "4. **Select Principal Components**: Sort the eigenvalues in descending order. Let's say that you find the first two principal components explain 90% of the total variance, which is satisfactory for your analysis.\n",
    "\n",
    "5. **Project Data onto Principal Components**: Transform your data by projecting it onto the first two principal components. The new dataset will have only two features, which are linear combinations of the original features. These two features capture most of the variance in the original data and can be used for further analysis, visualization, or machine learning.\n",
    "\n",
    "PCA helps you reduce the dimensionality of your data while preserving important patterns and reducing noise, making it a valuable technique for a wide range of applications in data analysis and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954dbeab-8b53-4f3e-b8a1-c162d8d290e1",
   "metadata": {},
   "source": [
    "#Q4\n",
    "\n",
    "PCA (Principal Component Analysis) is a dimensionality reduction technique that can also be used for feature extraction. Feature extraction is a broader concept that encompasses methods like PCA to reduce the dimensionality of data while retaining essential information. In other words, PCA is a specific technique used within the field of feature extraction.\n",
    "\n",
    "The relationship between PCA and feature extraction can be summarized as follows:\n",
    "\n",
    "1. **PCA as a Feature Extraction Technique**: PCA can be used as a feature extraction method to transform high-dimensional data into a lower-dimensional space, capturing the most important patterns and variations in the data. The principal components obtained from PCA are often used as the new features.\n",
    "\n",
    "2. **Feature Extraction's Broader Context**: Feature extraction, as a field, includes various techniques beyond PCA. These methods aim to reduce the number of features while preserving relevant information, simplifying data, and potentially improving the performance of machine learning models. PCA is one of the many feature extraction techniques available.\n",
    "\n",
    "Here's an example to illustrate how PCA can be used for feature extraction:\n",
    "\n",
    "Suppose you have a dataset of grayscale images, and each image is represented as a matrix of pixel values. Each image has thousands of pixels, making it a high-dimensional dataset, but you want to reduce the dimensionality while retaining the essential information to classify the images.\n",
    "\n",
    "1. **Data Preparation**: You start with a dataset of grayscale images, where each image has, for example, 1000x1000 pixels. Each pixel is considered a feature.\n",
    "\n",
    "2. **PCA as Feature Extraction**: You can apply PCA to the dataset to reduce its dimensionality. After PCA, you obtain a set of principal components, each of which is a linear combination of the original pixel features. These principal components capture the most significant variations in the images.\n",
    "\n",
    "3. **New Feature Representation**: The principal components obtained from PCA serve as new features. Instead of using the original pixel values, you represent each image using a reduced set of principal components. The number of principal components you choose to keep will depend on how much variance you want to retain.\n",
    "\n",
    "4. **Machine Learning**: You can use this reduced feature representation for machine learning tasks, such as image classification. The reduced feature space is often more manageable and can lead to faster training and improved model performance, especially when the original feature space is high-dimensional.\n",
    "\n",
    "By applying PCA as a feature extraction technique, you've effectively reduced the dimensionality of the image data while retaining the critical information needed for your classification task. This is a common approach in computer vision and various other domains where high-dimensional data can be challenging to work with directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ec3746-38e2-4987-8ea9-c52768174076",
   "metadata": {},
   "source": [
    "#Q5\n",
    "\n",
    "Min-Max scaling, also known as normalization, is a common preprocessing technique used to scale the features of a dataset to a specific range, typically between 0 and 1. It's particularly useful when the features in your dataset have different scales, and you want to bring them to a common scale to ensure they have equal weight in your recommendation system. In your project to build a recommendation system for a food delivery service with features like price, rating, and delivery time, you can use Min-Max scaling as follows:\n",
    "\n",
    "1. **Understand the Data**:\n",
    "   First, you should understand the range and distribution of each feature in your dataset. For example:\n",
    "   - Price may be measured in dollars, with a range from, say, $5 to $50.\n",
    "   - Rating might be on a scale from 1 to 5.\n",
    "   - Delivery time could be in minutes, with a range from, for instance, 20 to 60 minutes.\n",
    "\n",
    "2. **Choose the Scaling Range**:\n",
    "   Determine the range to which you want to scale your features. In most cases, Min-Max scaling scales features to the range [0, 1]. However, you can choose a different range if it's more suitable for your specific application.\n",
    "\n",
    "3. **Apply Min-Max Scaling**:\n",
    "   For each feature (price, rating, and delivery time), apply the Min-Max scaling formula for each data point:\n",
    "   \n",
    "   For Price:\n",
    "   \\[Price_{\\text{scaled}} = \\frac{Price - \\text{min(Price)}}{\\text{max(Price)} - \\text{min(Price)}}\\]\n",
    "\n",
    "   For Rating:\n",
    "   \\[Rating_{\\text{scaled}} = \\frac{Rating - \\text{min(Rating)}}{\\text{max(Rating)} - \\text{min(Rating)}}\\]\n",
    "\n",
    "   For Delivery Time:\n",
    "   \\[DeliveryTime_{\\text{scaled}} = \\frac{DeliveryTime - \\text{min(DeliveryTime)}}{\\text{max(DeliveryTime)} - \\text{min(DeliveryTime)}}\\]\n",
    "\n",
    "   In each case, \"min(feature)\" is the minimum value of that feature in your dataset, and \"max(feature)\" is the maximum value.\n",
    "\n",
    "4. **Use Scaled Features for Recommendation**:\n",
    "   Once you have applied Min-Max scaling to your features, you will have new feature values that fall within the specified range (e.g., [0, 1]). These scaled features can be used in your recommendation system. The scaled features will ensure that no single feature dominates the recommendation process due to its original scale.\n",
    "\n",
    "Min-Max scaling can help ensure that all your features are on a common scale, which is important for recommendation systems, as it allows you to make fair and meaningful comparisons between different items or restaurants based on their features. It can lead to more accurate and balanced recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce49a89a-d68c-4a88-92b1-7335dafae52d",
   "metadata": {},
   "source": [
    "#Q6\n",
    "\n",
    "Using Principal Component Analysis (PCA) to reduce the dimensionality of a dataset for predicting stock prices is a common and useful approach. Reducing dimensionality can help simplify the model, improve computational efficiency, and mitigate the curse of dimensionality. Here's a step-by-step guide on how to use PCA in this context:\n",
    "\n",
    "1. **Data Preprocessing**:\n",
    "   - **Data Cleaning**: Start by cleaning your dataset, handling missing values, and ensuring data consistency.\n",
    "   - **Feature Scaling**: Standardize or normalize your data, so that features with different scales do not bias the PCA results. Standardization (mean = 0, standard deviation = 1) is often a good choice in this context.\n",
    "\n",
    "2. **Feature Selection or Extraction**:\n",
    "   Decide whether to use PCA as feature extraction or feature selection. In the context of predicting stock prices, you might opt for feature extraction, where PCA will transform your original features into a smaller set of uncorrelated features (principal components).\n",
    "\n",
    "3. **Calculate Principal Components**:\n",
    "   Apply PCA to your dataset to obtain principal components. Here's how:\n",
    "   - Calculate the covariance matrix of your standardized or normalized features.\n",
    "   - Compute the eigenvalues and eigenvectors of the covariance matrix. The eigenvectors represent the principal components, and the eigenvalues indicate the amount of variance explained by each component.\n",
    "   - Sort the eigenvectors by their corresponding eigenvalues in descending order. The eigenvectors with higher eigenvalues capture more variance.\n",
    "\n",
    "4. **Choose the Number of Principal Components**:\n",
    "   Decide how many principal components you want to keep. This decision can be based on the cumulative explained variance. You might aim to retain a certain percentage of the total variance, such as 95% or 99%. To make this decision, calculate the cumulative explained variance for different numbers of components and choose the number that meets your criteria.\n",
    "\n",
    "5. **Project Data Onto Principal Components**:\n",
    "   Project your original data onto the selected principal components. Each data point will be represented by its values along the retained principal components.\n",
    "\n",
    "6. **Model Building**:\n",
    "   Train your stock price prediction model using the reduced feature set. You can use various machine learning algorithms, such as regression models or time series models, depending on the nature of your dataset and the problem.\n",
    "\n",
    "7. **Model Evaluation**:\n",
    "   Assess the performance of your stock price prediction model using appropriate evaluation metrics. You may need to fine-tune your model parameters or adjust the number of retained principal components based on the model's performance.\n",
    "\n",
    "8. **Interpretation**:\n",
    "   While PCA helps in dimensionality reduction, it also brings a trade-off in interpretability. Interpret the results of your model with respect to the original features and understand how the principal components relate to stock price predictions.\n",
    "\n",
    "It's important to note that PCA may not always improve the performance of a stock price prediction model. The choice of the number of principal components and the interpretation of the results require careful consideration and domain knowledge. You should also consider alternative dimensionality reduction techniques and evaluate their impact on your specific problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2628eb9-ca4d-43e1-b913-49da81905e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.        ]\n",
      " [0.21052632]\n",
      " [0.47368421]\n",
      " [0.73684211]\n",
      " [1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Q7\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max = MinMaxScaler(feature_range = (-1,1))\n",
    "data = [[1],[5],[10],[15],[20]]\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b2ec96-37e7-4334-9551-c3b0e81a775b",
   "metadata": {},
   "source": [
    "#Q8\n",
    "\n",
    "The number of principal components to retain when performing feature extraction using PCA depends on your specific objectives and the amount of variance you want to preserve. Here are the typical steps to decide how many principal components to keep:\n",
    "\n",
    "1. **Standardize or Normalize the Data**:\n",
    "   Before applying PCA, it's essential to standardize or normalize your data, particularly for features with different scales. This ensures that PCA isn't biased towards features with larger scales.\n",
    "\n",
    "2. **Calculate the Principal Components**:\n",
    "   Apply PCA to your dataset. The PCA process will yield a set of principal components.\n",
    "\n",
    "3. **Determine the Cumulative Explained Variance**:\n",
    "   After obtaining the principal components, calculate the cumulative explained variance for each number of retained components. The cumulative explained variance tells you how much of the total variance in the data is explained by the retained components.\n",
    "\n",
    "4. **Choose the Number of Components**:\n",
    "   The choice of the number of principal components to retain depends on your goals. Common strategies include:\n",
    "   - Retaining a certain percentage of the total variance: For instance, you might decide to retain enough components to explain 95% or 99% of the total variance. This ensures you capture most of the data's variability.\n",
    "   - Scree plot: Plot the explained variance against the number of components and look for an \"elbow\" in the plot. The \"elbow\" is where the explained variance starts to level off. You can choose the number of components just before this point.\n",
    "   - Domain knowledge: Consider whether you can achieve your objectives with a smaller number of components, especially if you're trying to reduce the dimensionality for computational reasons or improve model interpretability.\n",
    "\n",
    "5. **Retain Principal Components**:\n",
    "   Once you've made your decision, retain the specified number of principal components and use them for further analysis, modeling, or visualization.\n",
    "\n",
    "The appropriate number of principal components to retain can vary from one dataset and problem to another. It's often a trade-off between dimensionality reduction and the amount of information retained. You should consider your specific objectives, the impact on model performance, and domain knowledge when making this decision.\n",
    "\n",
    "It's also essential to note that in practice, it's common to start with a more generous number of components and then evaluate the impact on your specific task. You can iterate and fine-tune the number of components based on your model's performance and other considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875b1d5e-5b43-4c4e-8cb5-fb6191308508",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
